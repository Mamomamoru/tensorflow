{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_sampleCNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"MKRnT5HHurva","colab_type":"text"},"cell_type":"markdown","source":["## Convolutional Neural Network\n"]},{"metadata":{"id":"qoo7ElQfu0rH","colab_type":"text"},"cell_type":"markdown","source":["### Basic CNN \n","(fmap, size, kernel_size, strides, padding, activation)\n","- 入力 : (1, 28\\*28,- , -, -)\n","- C1 : (32, 28\\*28, 3\\*3, 1, SAME, ReLU)\n","- C2 : (64, 14\\*14, 3\\*3, 2, SAME, ReLU)\n","- S3 : (64, 7\\*7, 2\\*2, 2, VALID, -)\n","- F4 : (-, 64\\*64, -, -, -, ReLU)\n","- 出力 : (-, 10\\*10, -, -, -, softmax)"]},{"metadata":{"id":"AcvbetLBORFi","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import os\n","import tensorflow as tf\n","\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"sHYTm7w18usI","colab":{}},"cell_type":"code","source":["# MNIST\n","height = 28\n","width = 28\n","chanels = 1\n","n_inputs = height * width\n","\n","conv1_fmaps = 32\n","conv1_ksize = 3\n","conv1_stride = 1\n","conv1_pad = \"SAME\"\n","\n","conv2_fmaps = 64\n","conv2_ksize = 3\n","conv2_stride = 2\n","conv2_pad = \"SAME\"\n","\n","pool3_fmaps = conv2_fmaps\n","\n","n_fc1 = 64\n","n_outputs = 10\n","\n","reset_graph()\n","\n","with tf.name_scope(\"inputs\"):\n","    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n","    X_reshaped = tf.reshape(X, [-1, height, width, chanels])\n","    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n","    \n","conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n","                         strides=conv1_stride, padding=conv1_pad\n","                        )  # (28, 28)\n","\n","conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n","                         strides=conv2_stride, padding=conv2_pad\n","                        )  # (14, 14)\n","\n","with tf.name_scope(\"pool3\"):\n","    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")  # (7, 7)\n","    pool3_flat = tf.reshape(pool3, [-1, pool3_fmaps*7*7])\n","    \n","with tf.name_scope(\"fc1\"):\n","    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n","    \n","with tf.name_scope(\"outputs\"):\n","    logits = tf.layers.dense(fc1, n_outputs, name=\"logits\")\n","    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n","\n","with tf.name_scope(\"train\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","    optimizer = tf.train.AdamOptimizer()\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","with tf.name_scope(\"init_and_save\"):\n","    init = tf.global_variables_initializer()\n","    saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e0u8mbjZOZON","colab_type":"code","outputId":"c6e41362-c6b7-4b1a-cd5e-0390af93f1b0","executionInfo":{"status":"ok","timestamp":1548060165167,"user_tz":-540,"elapsed":12062,"user":{"displayName":"松永葵","photoUrl":"","userId":"06844649251918467809"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"cell_type":"code","source":["from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-c50d5bb4a85c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stdout"}]},{"metadata":{"id":"1egVhQn4suGq","colab_type":"code","outputId":"ff68ba55-bed9-410e-8e43-c9c9f9e489c7","executionInfo":{"status":"ok","timestamp":1547695157281,"user_tz":-540,"elapsed":599765,"user":{"displayName":"松永葵","photoUrl":"","userId":"06844649251918467809"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["import time\n","\n","n_epochs = 10\n","batch_size = 100\n","\n","with tf.Session() as sess:\n","    init.run()\n","    t0 = time.time()\n","    for epoch in range(n_epochs):\n","        for iteration in range(mnist.train.num_examples // batch_size):\n","            X_batch, y_batch = mnist.train.next_batch(batch_size)\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n","        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n","\n","        save_path = saver.save(sess, \"./my_mnist_model\")\n","        \n","    t1 = time.time()\n","    print(\"Total training time: {:.1f}s\".format(t1 - t0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 Train accuracy: 0.99 Test accuracy: 0.9783\n","1 Train accuracy: 0.97 Test accuracy: 0.9806\n","2 Train accuracy: 0.99 Test accuracy: 0.9849\n","3 Train accuracy: 1.0 Test accuracy: 0.9839\n","4 Train accuracy: 0.99 Test accuracy: 0.9856\n","5 Train accuracy: 0.99 Test accuracy: 0.9877\n","6 Train accuracy: 1.0 Test accuracy: 0.9856\n","7 Train accuracy: 1.0 Test accuracy: 0.9835\n","8 Train accuracy: 1.0 Test accuracy: 0.9863\n","9 Train accuracy: 1.0 Test accuracy: 0.9833\n","Total training time: 598.8s\n"],"name":"stdout"}]},{"metadata":{"id":"wjJOBaUTz8Vt","colab_type":"text"},"cell_type":"markdown","source":["### High Accuracy CNN\n","- **Dropout**\n","- **早期打ち切り** \\\\\n","(fmap, size, kernel_size, strides, padding, activation)\n","    - 入力 : (1, 28\\*28,- , -, -)\n","    - C1 : (32, 28\\*28, 3\\*3, 1, SAME, ReLU)\n","    - C2 : (64, 28\\*28, 3\\*3, 1, SAME, ReLU)\n","    - S3 : (64, 14\\*14, 2\\*2, 2, VALID, -)\n","        - dropout C2 (dropout_rate=0.25)\n","    - F4 : (-, 128\\*128, -, -, -, ReLU)\n","        - dropout F4 (dropout_rate=0.5)\n","    - 出力 : (-, 10\\*10, -, -, -, softmax)"]},{"metadata":{"id":"vsH3qpV4tND1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"957017d9-60b6-4868-a8cd-2abf772161f4","executionInfo":{"status":"error","timestamp":1548060263292,"user_tz":-540,"elapsed":483,"user":{"displayName":"松永葵","photoUrl":"","userId":"06844649251918467809"}}},"cell_type":"code","source":["# 構築\n","# MNIST\n","height = 28\n","width = 28\n","chanels = 1\n","n_inputs = height * width\n","\n","conv1_fmaps = 32\n","conv1_ksize = 3\n","conv1_stride = 1\n","conv1_pad = \"SAME\"\n","\n","conv2_fmaps = 64\n","conv2_ksize = 3\n","conv2_stride = 1\n","conv2_pad = \"SAME\"\n","conv2_dropout_rate = 0.25\n","\n","pool3_fmaps = conv2_fmaps\n","\n","n_fc1 = 128\n","fc1_dropout_rate = 0.5\n","\n","n_outputs = 10\n","\n","reset_graph()\n","\n","with tf.name_scope(\"inputs\"):\n","    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n","    X_reshaped = tf.reshape(X, [-1, height, width, chanels])\n","    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n","    training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n","\n","conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n","                         strides=conv1_stride, padding=conv1_pad,\n","                         activation=tf.nn.relu, name=\"conv1\")  # (28, 28)\n","conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n","                         strides=conv2_stride, padding=conv2_pad,\n","                         activation=tf.nn.relu, name=\"conv2\")  # (28, 28)\n","\n","with tf.name_scope(\"pool3\"):\n","    pool3 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")  # (14, 14)\n","    pool3_flat = tf.reshape(pool3, [-1, pool3_fmaps*14*14])\n","    pool3_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\n","    \n","with tf.name_scope(\"fc1\"):\n","    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")  # 128\n","    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training\n","    \n","with tf.name_scope(\"outputs\"):\n","    logits = tf.layers.dense(fc1_drop, n_outputs, name=\"logits\") # 10\n","    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n","    \n","with tf.name_scope(\"train\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","    optimizer = tf.train.AdamOptimizer()\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","with tf.name_scope(\"init_and_save\"):\n","    init = tf.global_variables_initializer()\n","    saver = tf.train.Saver()"],"execution_count":9,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-90683714caff>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    with tf.name_scope(\"outputs\"):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"id":"ZleryL_Z9dTo","colab_type":"code","colab":{}},"cell_type":"code","source":["# モデルの保存と復元用の関数\n","def get_model_params():\n","    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n","    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n","\n","def restore_model_params(model_params):\n","    gvar_names = list(model_params.keys())\n","    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n","                  for gvar_name in gvar_names}\n","    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n","    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n","    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"29ns72GE_K9A","colab_type":"code","outputId":"43d90b14-6bb3-4dc1-fae3-cd38b77a8cc6","executionInfo":{"status":"ok","timestamp":1547702756281,"user_tz":-540,"elapsed":1031294,"user":{"displayName":"松永葵","photoUrl":"","userId":"06844649251918467809"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"cell_type":"code","source":["import time\n","\n","# 実行\n","n_epochs = 1000\n","batch_size = 50\n","\n","# 早期打ち切り\n","best_loss_val = np.infty\n","check_interval = 500\n","checks_since_last_progress = 0\n","max_checks_without_progress = 20\n","best_model_params = None\n","\n","with tf.Session() as sess:\n","    init.run()\n","    t0 = time.time()\n","    for epoch in range(n_epochs):\n","        for iteration in range(mnist.train.num_examples // batch_size):\n","            X_batch, y_batch = mnist.train.next_batch(batch_size)\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n","            if iteration % check_interval == 0:\n","                loss_val = loss.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n","                if loss_val < best_loss_val:\n","                    best_loss_val = loss_val\n","                    checks_since_last_progress = 0\n","                    best_model_params = get_model_params()\n","                else:\n","                    checks_since_last_progress += 1\n","        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n","        print(\"Epoch {}, train accuracy {:.4f}%, valid. accuracy {:.4f}%, valid. best loss {:.6f}%\".format(\n","            epoch, acc_train*100, acc_val*100, best_loss_val))\n","        if checks_since_last_progress > max_checks_without_progress:\n","            t1 = time.time()\n","            print(\"Early stopping!\")\n","            print(\"Total training time: {:.1f}s\".format(t1 - t0))\n","            break\n","                \n","    if best_model_params:\n","        restore_model_params(best_model_params)\n","    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n","    print(\"Final accuracy on test set :\", acc_test)\n","    save_path = saver.save(sess, \"./my_mnist_model\")\n","                    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0, train accuracy 96.0000%, valid. accuracy 98.4400%, valid. best loss 0.058560%\n","Epoch 1, train accuracy 100.0000%, valid. accuracy 98.9200%, valid. best loss 0.042897%\n","Epoch 2, train accuracy 100.0000%, valid. accuracy 98.8800%, valid. best loss 0.042156%\n","Epoch 3, train accuracy 98.0000%, valid. accuracy 99.0200%, valid. best loss 0.036897%\n","Epoch 4, train accuracy 100.0000%, valid. accuracy 98.9800%, valid. best loss 0.035211%\n","Epoch 5, train accuracy 100.0000%, valid. accuracy 99.2600%, valid. best loss 0.032130%\n","Epoch 6, train accuracy 100.0000%, valid. accuracy 99.0400%, valid. best loss 0.029772%\n","Epoch 7, train accuracy 100.0000%, valid. accuracy 99.1800%, valid. best loss 0.029772%\n","Epoch 8, train accuracy 100.0000%, valid. accuracy 99.2000%, valid. best loss 0.029772%\n","Epoch 9, train accuracy 100.0000%, valid. accuracy 99.1600%, valid. best loss 0.029772%\n","Epoch 10, train accuracy 100.0000%, valid. accuracy 99.2800%, valid. best loss 0.029772%\n","Epoch 11, train accuracy 100.0000%, valid. accuracy 99.1000%, valid. best loss 0.029772%\n","Epoch 12, train accuracy 100.0000%, valid. accuracy 99.3000%, valid. best loss 0.029772%\n","Epoch 13, train accuracy 100.0000%, valid. accuracy 99.1400%, valid. best loss 0.029772%\n","Early stopping!\n","Final accuracy on test set : 0.9903\n"],"name":"stdout"}]},{"metadata":{"id":"_7aeDwe0Efik","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}